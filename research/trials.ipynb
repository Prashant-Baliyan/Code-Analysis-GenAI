{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clone github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Ineuron\\\\Code-Analysis-GenAI\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file test_repo already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "#Repo.clone_from(\"https://github.com/Prashant-Baliyan/DVC-NLP-Project-with-docs\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path+'src',\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\src\\\\stage_00_template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import argparse\\nimport os\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport logging\\nfrom src.utils import read_yaml, create_directories\\nimport random\\n\\nSTAGE = \"Featurization\" ## <<< change stage name \\n\\nlogging.basicConfig(\\n    filename=os.path.join(\"logs\", \\'running_logs.log\\'), \\n    level=logging.INFO, \\n    format=\"[%(asctime)s: %(levelname)s: %(module)s]: %(message)s\",\\n    filemode=\"a\"\\n    )\\n\\ndef main(config_path, params_path):\\n    pass\\n\\nif __name__ == \\'__main__\\':\\n    args = argparse.ArgumentParser()\\n    args.add_argument(\"--config\", \"-c\", default=\"configs/config.yaml\")\\n    args.add_argument(\"--params\", \"-p\", default=\"params.yaml\")\\n    parsed_args = args.parse_args()\\n\\n    try:\\n        logging.info(\"\\\\n********************\")\\n        logging.info(f\">>>>> stage {STAGE} started <<<<<\")\\n        main(config_path=parsed_args.config, params_path=parsed_args.params)\\n        logging.info(f\">>>>> stage {STAGE} completed!<<<<<\\\\n\")\\n    except Exception as e:\\n        logging.exception(e)\\n        raise e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\stage_01_prepare_data.py', 'language': <Language.PYTHON: 'python'>}, page_content='import argparse\\nimport os\\nimport logging\\nfrom src.utils import read_yaml, create_directories\\nfrom src.utils.data_mgmt import process_posts\\nimport random\\n\\n\\nSTAGE = \"Prepare_data\" ## <<< change stage name \\n\\nlogging.basicConfig(\\n    filename=os.path.join(\"logs\", \\'running_logs.log\\'), \\n    level=logging.INFO, \\n    format=\"[%(asctime)s: %(levelname)s: %(module)s]: %(message)s\",\\n    filemode=\"a\"\\n    )\\n\\n\\ndef main(config_path, params_path):\\n    ## read config files\\n    config = read_yaml(config_path)\\n    params = read_yaml(params_path)\\n    \\n    source_data_dir = config[\"source_data\"][\"data_dir\"]\\n    source_data_file = config[\"source_data\"][\"data_file\"]\\n    source_data_path = os.path.join(source_data_dir, source_data_file)\\n\\n    split = params[\"prepare\"][\"split\"] # split ratio\\n    seed = params[\"prepare\"][\"seed\"]\\n    tag = params[\"prepare\"][\"tag\"]\\n\\n    random.seed(seed)\\n\\n    artifacts = config[\"artifacts\"]\\n    prepare_data_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], artifacts[\"PREPARED_DATA\"])\\n    create_directories([prepare_data_dir_path])\\n\\n    train_data_path = os.path.join(prepare_data_dir_path,artifacts[\"TRAIN_DATA\"])\\n    test_data_path = os.path.join(prepare_data_dir_path,artifacts[\"TEST_DATA\"])\\n\\n    encode = \"utf8\"\\n    with open(source_data_path, encoding=encode) as fd_in: # actual input data that we are reading\\n        with open(train_data_path, \"w\", encoding=encode) as fd_out_train: # writing train data\\n            with open(test_data_path, \"w\", encoding=encode) as fd_out_test: # writing test data\\n                process_posts(fd_in, fd_out_train, fd_out_test, tag, split)\\n    \\nif __name__ == \\'__main__\\':\\n    args = argparse.ArgumentParser()\\n    args.add_argument(\"--config\", \"-c\", default=\"configs/config.yaml\")\\n    args.add_argument(\"--params\", \"-p\", default=\"params.yaml\")\\n    parsed_args = args.parse_args()\\n\\n    try:\\n        logging.info(\"\\\\n********************\")\\n        logging.info(f\">>>>> stage {STAGE} started <<<<<\")\\n        main(config_path=parsed_args.config, params_path=parsed_args.params)\\n        logging.info(f\">>>>> stage {STAGE} completed!<<<<<\\\\n\")\\n    except Exception as e:\\n        logging.exception(e)\\n        raise e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\stage_02_featurization.py', 'language': <Language.PYTHON: 'python'>}, page_content='import argparse\\nimport os\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport logging\\nfrom src.utils import read_yaml, create_directories, get_df, save_matrix\\nimport random\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\\n\\nSTAGE = \"Featurization\" ## <<< change stage name \\n\\nlogging.basicConfig(\\n    filename=os.path.join(\"logs\", \\'running_logs.log\\'), \\n    level=logging.INFO, \\n    format=\"[%(asctime)s: %(levelname)s: %(module)s]: %(message)s\",\\n    filemode=\"a\"\\n    )\\n\\n\\ndef main(config_path, params_path):\\n    ## read config files\\n    config = read_yaml(config_path)\\n    params = read_yaml(params_path)\\n\\n    artifacts = config[\"artifacts\"]\\n    prepare_data_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], artifacts[\"PREPARED_DATA\"])\\n    train_data_path = os.path.join(prepare_data_dir_path,artifacts[\"TRAIN_DATA\"])\\n    test_data_path = os.path.join(prepare_data_dir_path,artifacts[\"TEST_DATA\"])\\n\\n    featurized_data_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], artifacts[\"FEATURIZED_DATA\"])\\n    create_directories([featurized_data_dir_path])\\n\\n    featurized_train_data_path = os.path.join(featurized_data_dir_path, artifacts[\"FEATURIZED_DATA_TRAIN\"])\\n    featurized_test_data_path = os.path.join(featurized_data_dir_path, artifacts[\"FEATURIZED_DATA_TEST\"])\\n\\n    max_features = params[\"featurize\"][\"max_features\"]\\n    n_grams = params[\"featurize\"][\"n_grams\"]\\n\\n    # train\\n    df_train = get_df(train_data_path)\\n    train_words = np.array(df_train.text.str.lower().values.astype(\"U\"))\\n\\n    bag_of_words = CountVectorizer(\\n        stop_words=\"english\",\\n        max_features=max_features,\\n        ngram_range=(1, n_grams)\\n    )\\n\\n    bag_of_words.fit(train_words)\\n    train_words_binary_matrix = bag_of_words.transform(train_words)\\n\\n    tfidf = TfidfTransformer(smooth_idf=False)\\n    tfidf.fit(train_words_binary_matrix)\\n    train_words_tfidf_matrix = tfidf.transform(train_words_binary_matrix)\\n    # call a function to save this matrix\\n    save_matrix(df=df_train, text_matrix=train_words_tfidf_matrix, out_path=featurized_train_data_path)\\n\\n    # for test data\\n    df_test = get_df(test_data_path)\\n    test_words = np.array(df_test.text.str.lower().values.astype(\"U\"))\\n    test_words_binary_matrix = bag_of_words.transform(test_words)\\n    test_words_tfidf_matrix = tfidf.transform(test_words_binary_matrix)\\n    # call a function to save this matrix\\n    save_matrix(df=df_test, text_matrix=test_words_tfidf_matrix, out_path=featurized_test_data_path)\\n    \\n\\nif __name__ == \\'__main__\\':\\n    args = argparse.ArgumentParser()\\n    args.add_argument(\"--config\", \"-c\", default=\"configs/config.yaml\")\\n    args.add_argument(\"--params\", \"-p\", default=\"params.yaml\")\\n    parsed_args = args.parse_args()\\n\\n    try:\\n        logging.info(\"\\\\n********************\")\\n        logging.info(f\">>>>> stage {STAGE} started <<<<<\")\\n        main(config_path=parsed_args.config, params_path=parsed_args.params)\\n        logging.info(f\">>>>> stage {STAGE} completed!<<<<<\\\\n\")\\n    except Exception as e:\\n        logging.exception(e)\\n        raise e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\stage_03_train.py', 'language': <Language.PYTHON: 'python'>}, page_content='import argparse\\nimport os\\nimport shutil\\nfrom tqdm import tqdm\\nimport logging\\nfrom src.utils.common import read_yaml, create_directories\\nimport joblib\\nimport numpy as np\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\n\\nSTAGE = \"Training\" ## <<< change stage name \\n\\nlogging.basicConfig(\\n    filename=os.path.join(\"logs\", \\'running_logs.log\\'), \\n    level=logging.INFO, \\n    format=\"[%(asctime)s: %(levelname)s: %(module)s]: %(message)s\",\\n    filemode=\"a\"\\n    )\\n\\n\\ndef main(config_path, params_path):\\n    ## read config files\\n    config = read_yaml(config_path)\\n    params = read_yaml(params_path)\\n\\n    artifacts = config[\"artifacts\"]\\n    featurized_data_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], artifacts[\"FEATURIZED_DATA\"])\\n    featurized_train_data_path = os.path.join(featurized_data_dir_path, artifacts[\"FEATURIZED_DATA_TRAIN\"])\\n\\n    model_dir = artifacts[\"MODEL_DIR\"]\\n    model_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], model_dir)\\n    create_directories([model_dir_path])\\n    model_name = artifacts[\"MODEL_NAME\"]\\n    model_path = os.path.join(model_dir_path, model_name)\\n\\n    matrix = joblib.load(featurized_train_data_path)\\n\\n    labels = np.squeeze(matrix[:, 1].toarray())\\n    X = matrix[:, 2:]\\n\\n    logging.info(f\"input matrix size: {matrix.shape}\")\\n    logging.info(f\"X matrix size: {X.shape}\")\\n    logging.info(f\"Y matrix size or labels: {labels.shape}\")\\n\\n    seed = params[\"train\"][\"seed\"]\\n    n_est = params[\"train\"][\"n_est\"]\\n    n_jobs = params[\"train\"][\"n_jobs\"]\\n    min_split = params[\"train\"][\"min_split\"]\\n\\n    model = RandomForestClassifier(\\n        n_estimators=n_est,\\n        min_samples_split=min_split,\\n        n_jobs=n_jobs,\\n        random_state=seed\\n    )\\n    model.fit(X, labels)\\n\\n    joblib.dump(model, model_path)\\n    logging.info(f\"saved our model at: {model_path}\")\\n    \\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    args = argparse.ArgumentParser()\\n    args.add_argument(\"--config\", \"-c\", default=\"configs/config.yaml\")\\n    args.add_argument(\"--params\", \"-p\", default=\"params.yaml\")\\n    parsed_args = args.parse_args()\\n\\n    try:\\n        logging.info(\"\\\\n********************\")\\n        logging.info(f\">>>>> stage {STAGE} started <<<<<\")\\n        main(config_path=parsed_args.config, params_path=parsed_args.params)\\n        logging.info(f\">>>>> stage {STAGE} completed!<<<<<\\\\n\")\\n    except Exception as e:\\n        logging.exception(e)\\n        raise e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\stage_04_evaluate.py', 'language': <Language.PYTHON: 'python'>}, page_content='import argparse\\nimport os\\nimport shutil\\nfrom tqdm import tqdm\\nimport logging\\nfrom src.utils import read_yaml, save_json\\nimport sklearn.metrics as metrics\\nimport math\\nimport numpy as np\\nimport joblib\\n\\nSTAGE = \"Evaluation\" ## <<< change stage name \\n\\nlogging.basicConfig(\\n    filename=os.path.join(\"logs\", \\'running_logs.log\\'), \\n    level=logging.INFO, \\n    format=\"[%(asctime)s: %(levelname)s: %(module)s]: %(message)s\",\\n    filemode=\"a\"\\n    )\\n\\n\\ndef main(config_path, params_path):\\n    ## read config files\\n    config = read_yaml(config_path)\\n    params = read_yaml(params_path)\\n    \\n    artifacts = config[\"artifacts\"]\\n\\n    featurized_data_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], artifacts[\"FEATURIZED_DATA\"])\\n    featurized_test_data_path = os.path.join(featurized_data_dir_path, artifacts[\"FEATURIZED_DATA_TEST\"])\\n\\n    model_dir = artifacts[\"MODEL_DIR\"]\\n    model_dir_path = os.path.join(artifacts[\"ARTIFACTS_DIR\"], model_dir)\\n    model_name = artifacts[\"MODEL_NAME\"]\\n    model_path = os.path.join(model_dir_path, model_name)\\n\\n    model = joblib.load(model_path)\\n    matrix = joblib.load(featurized_test_data_path)\\n\\n    labels = np.squeeze(matrix[:, 1].toarray())\\n    X = matrix[:, 2:]\\n\\n    predictions_probabilities = model.predict_proba(X)\\n    pred = predictions_probabilities[:, 1]\\n    \\n    logging.info(f\"labels, predictions: {list(zip(labels, pred))}\")\\n\\n    PRC_json_path = config[\"plots\"][\"PRC\"]\\n    ROC_json_path = config[\"plots\"][\"ROC\"]\\n    scores_json_path = config[\"metrics\"][\"SCORES\"]\\n\\n    avg_prec = metrics.average_precision_score(labels, pred)\\n    roc_auc = metrics.roc_auc_score(labels, pred)\\n\\n    logging.info(f\"len of labels: {len(labels)} and predictions: {len(pred)}\")\\n    scores = {\\n        \"avg_prec\": avg_prec,\\n        \"roc_auc\":roc_auc\\n    }\\n\\n    save_json(scores_json_path, scores)\\n\\n    precision, recall, prc_threshold = metrics.precision_recall_curve(labels, pred)\\n\\n    nth_point = math.ceil(len(prc_threshold)/1000)\\n    prc_points = list(zip(precision, recall, prc_threshold))[::nth_point]\\n    # prc_points = list(zip(precision, recall, prc_threshold))\\n\\n    logging.info(f\"no. of prc points: {len(prc_points)}\")\\n    # logging.info(f\"precision: {precision}, \\\\nrecall: {recall}, \\\\nprc_threshold: {prc_threshold}\")\\n\\n    prc_data = {\\n        \"prc\":[\\n            {\"precision\": p, \"recall\": r, \"threshold\": t}\\n            for p, r, t in prc_points\\n        ]\\n    }\\n    \\n    save_json(PRC_json_path, prc_data)\\n\\n    fpr, tpr, roc_threshold = metrics.roc_curve(labels, pred)\\n    roc_points = zip(fpr, tpr, roc_threshold)\\n    roc_data = {\\n        \"roc\": [\\n            {\"fpr\": fp, \"tpr\": tp, \"threshold\": t}\\n            for fp, tp, t in roc_points\\n        ]\\n    }\\n    logging.info(f\"no. of roc points: {len(list(roc_points))}\")\\n    # logging.info(f\"fpr: {fpr}, \\\\ntpr: {tpr}, \\\\nroc_threshold: {roc_threshold}\")\\n\\n    save_json(ROC_json_path, roc_data)\\n\\n\\nif __name__ == \\'__main__\\':\\n    args = argparse.ArgumentParser()\\n    args.add_argument(\"--config\", \"-c\", default=\"configs/config.yaml\")\\n    args.add_argument(\"--params\", \"-p\", default=\"params.yaml\")\\n    parsed_args = args.parse_args()\\n\\n    try:\\n        logging.info(\"\\\\n********************\")\\n        logging.info(f\">>>>> stage {STAGE} started <<<<<\")\\n        main(config_path=parsed_args.config, params_path=parsed_args.params)\\n        logging.info(f\">>>>> stage {STAGE} completed!<<<<<\\\\n\")\\n    except Exception as e:\\n        logging.exception(e)\\n        raise e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport yaml\\nimport logging\\nimport time\\nimport pandas as pd\\nimport json\\n\\ndef read_yaml(path_to_yaml: str) -> dict:\\n    with open(path_to_yaml) as yaml_file:\\n        content = yaml.safe_load(yaml_file)\\n    logging.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n    return content\\n\\ndef create_directories(path_to_directories: list) -> None:\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        logging.info(f\"created directory at: {path}\")\\n\\n\\ndef save_json(path: str, data: dict) -> None:\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logging.info(f\"json file saved at: {path}\")\\n\\n\\ndef get_df(\\n    path_to_data: str, \\n    sep: str=\"\\\\t\", \\n    # column_names: list=[\"id\", \"label\", \"text\"],\\n    encoding=\\'utf-8\\') -> pd.DataFrame:\\n    df = pd.read_csv(\\n        path_to_data, \\n        delimiter=sep, \\n        encoding=encoding, \\n        # header=None, \\n        # names=column_names,\\n    )\\n    logging.info(f\"The input data frame {path_to_data} of size {df.shape} is read.\")\\n    return df'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\data_mgmt.py', 'language': <Language.PYTHON: 'python'>}, page_content='import logging\\nfrom tqdm import tqdm\\nimport random\\nimport xml.etree.ElementTree as ET\\nimport re\\nimport joblib\\nimport numpy as np\\nimport scipy.sparse as sparse\\n\\ndef process_posts(fd_in, fd_out_train, fd_out_test, target_tag, split):\\n    line_num = 1\\n    column_names = \"pid\\\\tlabel\\\\ttext\\\\n\"\\n    fd_out_train.write(column_names)\\n    fd_out_test.write(column_names)\\n    for line in tqdm(fd_in):\\n        try:\\n            fd_out = fd_out_train if random.random() > split else fd_out_test\\n\\n            attr = ET.fromstring(line).attrib ## getting the tags\\n\\n            pid = attr.get(\\'Id\\', \"\")\\n            label = 1 if target_tag in attr.get(\\'Tags\\', \"\") else 0\\n            title = re.sub(r\"\\\\s+\", \" \", attr.get(\\'Title\\', \"\")).strip()\\n            body = re.sub(r\"\\\\s+\", \" \", attr.get(\\'Body\\', \"\")).strip()\\n            text = f\"{title} {body}\" # title + \" \" + body\\n\\n            fd_out.write(f\"{pid}\\\\t{label}\\\\t{text}\\\\n\")\\n            line_num += 1\\n        except Exception as e:\\n            msg = f\"Skipping the broken line {line_num}: {e}\\\\n\"\\n            logging.exception(msg)\\n\\n\\ndef save_matrix(df, text_matrix, out_path):\\n    pid_matrix = sparse.csr_matrix(df.pid.astype(np.int64)).T\\n    label_matrix = sparse.csr_matrix(df.label.astype(np.int64)).T\\n\\n    result = sparse.hstack([pid_matrix, label_matrix, text_matrix], format=\"csr\")\\n\\n    msg = f\"The output matrix saved at {out_path} of shape: {result.shape}\"\\n    logging.info(msg)\\n    joblib.dump(result, out_path) '),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.utils.common import *\\nfrom src.utils.data_mgmt import *')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = document_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeeding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbpra\\AppData\\Local\\Temp\\ipykernel_14060\\3419084168.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\pbpra\\anaconda3\\envs\\llama_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledgebase chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbpra\\AppData\\Local\\Temp\\ipykernel_14060\\1944247624.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding = embeddings, persist_directory = './data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "#llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-2-7b-chat-hf\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "\n",
    "\n",
    "llm = CTransformers(model='D:\\Ineuron\\Code-Analysis-GenAI\\model\\llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "                    model_type='llama',\n",
    "                    config={'max_new_tokens': 128,\n",
    "                            'temperature': 0.01}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbpra\\AppData\\Local\\Temp\\ipykernel_14060\\1764516168.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"what is stage_02_featurization all about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbpra\\AppData\\Local\\Temp\\ipykernel_14060\\79176006.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n",
      "Number of tokens (875) exceeded maximum context length (512).\n"
     ]
    }
   ],
   "source": [
    "result = qa.invoke(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
